{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0         author_id    author_name          date  emails  \\\n0           0       @Acutoronto     Acutoronto  Mar 27, 2022     0.0   \n1           1     @LorraineZiff  Lorraine Ziff  Mar 26, 2022     0.0   \n2           2   @ImpactWellness    Chris Caito  Mar 27, 2022     0.0   \n3           3  @hiromiyoshihair   Hiro Miyoshi  Mar 27, 2022     0.0   \n4           4   @lunaxbrightwin   Liz lvs Joy   Mar 27, 2022     0.0   \n\n                                            hashtags language  likes  \\\n0  motivation,acutoronto,acupuncture,tcm,fertilit...       en      0   \n1  lorraineziff,dearlorraine,smile,friendship,lua...       en      5   \n2  MuhammadAli,john316,isaiah4031,philippians413,...       en      0   \n3                       mothersday,grateful,thankful       en      0   \n4                       WildsideOutNow,JOY,RedVelvet       en      6   \n\n          location   mentions  ...  n_all_caps  n_excl_marks n_words n_urls  \\\n0              NaN        NaN  ...           0             0      22      0   \n1              NaN  larryziff  ...           0             1      24      1   \n2              NaN        NaN  ...           2             0      24      0   \n3  London, England        NaN  ...           1             1      18      0   \n4              NaN  RVsmtown0  ...          14             0      19      0   \n\n  n_mentions n_hashtags                                      text_semantic  \\\n0          0         19      sunday inspirationalquotes reproductivehealth   \n1          1         18                      luau ready marinemax sarasota   \n2          0         11  hated every minute training said quit suffer l...   \n3          0          3  mother greateful wishing happiest day hiromiyo...   \n4          1          3  joy eres la mujer talentosa que existe red vel...   \n\n   n_nouns  n_verbs  n_adjectives  \n0       19        1             2  \n1       22        0             2  \n2       19        3             1  \n3       12        2             3  \n4       12        1             3  \n\n[5 rows x 26 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>author_id</th>\n      <th>author_name</th>\n      <th>date</th>\n      <th>emails</th>\n      <th>hashtags</th>\n      <th>language</th>\n      <th>likes</th>\n      <th>location</th>\n      <th>mentions</th>\n      <th>...</th>\n      <th>n_all_caps</th>\n      <th>n_excl_marks</th>\n      <th>n_words</th>\n      <th>n_urls</th>\n      <th>n_mentions</th>\n      <th>n_hashtags</th>\n      <th>text_semantic</th>\n      <th>n_nouns</th>\n      <th>n_verbs</th>\n      <th>n_adjectives</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@Acutoronto</td>\n      <td>Acutoronto</td>\n      <td>Mar 27, 2022</td>\n      <td>0.0</td>\n      <td>motivation,acutoronto,acupuncture,tcm,fertilit...</td>\n      <td>en</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>22</td>\n      <td>0</td>\n      <td>0</td>\n      <td>19</td>\n      <td>sunday inspirationalquotes reproductivehealth</td>\n      <td>19</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>@LorraineZiff</td>\n      <td>Lorraine Ziff</td>\n      <td>Mar 26, 2022</td>\n      <td>0.0</td>\n      <td>lorraineziff,dearlorraine,smile,friendship,lua...</td>\n      <td>en</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>larryziff</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>24</td>\n      <td>1</td>\n      <td>1</td>\n      <td>18</td>\n      <td>luau ready marinemax sarasota</td>\n      <td>22</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>@ImpactWellness</td>\n      <td>Chris Caito</td>\n      <td>Mar 27, 2022</td>\n      <td>0.0</td>\n      <td>MuhammadAli,john316,isaiah4031,philippians413,...</td>\n      <td>en</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>2</td>\n      <td>0</td>\n      <td>24</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>hated every minute training said quit suffer l...</td>\n      <td>19</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>@hiromiyoshihair</td>\n      <td>Hiro Miyoshi</td>\n      <td>Mar 27, 2022</td>\n      <td>0.0</td>\n      <td>mothersday,grateful,thankful</td>\n      <td>en</td>\n      <td>0</td>\n      <td>London, England</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>mother greateful wishing happiest day hiromiyo...</td>\n      <td>12</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>@lunaxbrightwin</td>\n      <td>Liz lvs Joy</td>\n      <td>Mar 27, 2022</td>\n      <td>0.0</td>\n      <td>WildsideOutNow,JOY,RedVelvet</td>\n      <td>en</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>RVsmtown0</td>\n      <td>...</td>\n      <td>14</td>\n      <td>0</td>\n      <td>19</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>joy eres la mujer talentosa que existe red vel...</td>\n      <td>12</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 26 columns</p>\n</div>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweet_data.csv\")\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "n_repeats = 2\n",
    "rskf = RepeatedStratifiedKFold(n_repeats=n_repeats, n_splits=n_splits, random_state=1410)\n",
    "\n",
    "clfs = {\"MNB\" : MultinomialNB(), \"SVM\" : SVC(random_state=1234)}\n",
    "\n",
    "scores = np.zeros(shape=(3, len(clfs), n_repeats * n_splits))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metoda 1 -- TF-IDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "X = df['text_semantic'].apply(lambda txt: np.str_(txt))\n",
    "y = df['sentiment'].map({'negative' : 0, 'neutral' : 1, 'positive' : 2}).astype(int).to_numpy()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for clf_id, clf_name in enumerate(clfs):\n",
    "    for fold_id, (train_idx, test_idx) in enumerate(rskf.split(X, y)):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train = vectorizer.fit_transform(X[train_idx])\n",
    "        clf = clone(clfs[clf_name])\n",
    "        y_pred = clf.fit(X_train, y[train_idx]).predict(vectorizer.transform(X[test_idx]))\n",
    "        scores[0, clf_id, fold_id] = accuracy_score(y[test_idx], y_pred)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metoda 2 -- metadane"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "X = df[['n_words', 'n_nouns', 'n_verbs', 'n_adjectives', 'n_all_caps', 'n_excl_marks', 'n_hashtags', 'n_mentions', 'n_urls', 'emails', 'quotes', 'retweets', 'likes']].astype(int).to_numpy()\n",
    "y = df['sentiment'].map({'negative' : 0, 'neutral' : 1, 'positive' : 2}).astype(int).to_numpy()\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# MNB\n",
    "for fold_id, (train_idx, test_idx) in enumerate(rskf.split(X, y)):\n",
    "    clf = clone(clfs['MNB'])\n",
    "    y_pred = clf.fit(X[train_idx], y[train_idx]).predict(X[test_idx])\n",
    "    scores[1, 0, fold_id] = accuracy_score(y[test_idx], y_pred)\n",
    "\n",
    "# SVM\n",
    "for fold_id, (train_idx, test_idx) in enumerate(rskf.split(X, y)):\n",
    "    clf = clone(clfs['SVM'])\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X[train_idx])\n",
    "    y_pred = clf.fit(X_train, y[train_idx]).predict(scaler.transform(X[test_idx]))\n",
    "    scores[1, 1, fold_id] = accuracy_score(y[test_idx], y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metoda 3 -- mieszana"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "X = df['text'].apply(lambda txt: np.str_(txt))\n",
    "y = df['sentiment'].map({'negative' : 0, 'neutral' : 1, 'positive' : 2}).astype(int).to_numpy()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for clf_id, clf_name in enumerate(clfs):\n",
    "    for fold_id, (train_idx, test_idx) in enumerate(rskf.split(X, y)):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train = vectorizer.fit_transform(X[train_idx])\n",
    "        clf = clone(clfs[clf_name])\n",
    "        y_pred = clf.fit(X_train, y[train_idx]).predict(vectorizer.transform(X[test_idx]))\n",
    "        scores[2, clf_id, fold_id] = accuracy_score(y[test_idx], y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.67503693 0.67060561 0.67751479 0.67307692 0.67307692 0.70162482\n",
      "   0.64106352 0.66568047 0.66272189 0.68195266]\n",
      "  [0.6661743  0.64844904 0.66863905 0.66715976 0.63757396 0.68537666\n",
      "   0.65140325 0.66715976 0.6464497  0.64349112]]\n",
      "\n",
      " [[0.41506647 0.41063516 0.48224852 0.4112426  0.44230769 0.42245199\n",
      "   0.40915805 0.43934911 0.43195266 0.44230769]\n",
      "  [0.52732644 0.5155096  0.52218935 0.54289941 0.52810651 0.52584934\n",
      "   0.53175775 0.55029586 0.49112426 0.52662722]]\n",
      "\n",
      " [[0.67208272 0.68094535 0.67307692 0.66863905 0.67307692 0.6957164\n",
      "   0.63367799 0.66863905 0.65088757 0.66715976]\n",
      "  [0.67651403 0.67208272 0.64349112 0.65828402 0.65236686 0.68094535\n",
      "   0.64106352 0.69674556 0.64940828 0.65680473]]]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}