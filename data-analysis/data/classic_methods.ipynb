{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0         author_id    author_name          date  emails  \\\n0           0       @Acutoronto     Acutoronto  Mar 27, 2022     0.0   \n1           1     @LorraineZiff  Lorraine Ziff  Mar 26, 2022     0.0   \n2           2   @ImpactWellness    Chris Caito  Mar 27, 2022     0.0   \n3           3  @hiromiyoshihair   Hiro Miyoshi  Mar 27, 2022     0.0   \n4           4   @lunaxbrightwin   Liz lvs Joy   Mar 27, 2022     0.0   \n\n                                            hashtags language  likes  \\\n0  motivation,acutoronto,acupuncture,tcm,fertilit...       en      0   \n1  lorraineziff,dearlorraine,smile,friendship,lua...       en      5   \n2  MuhammadAli,john316,isaiah4031,philippians413,...       en      0   \n3                       mothersday,grateful,thankful       en      0   \n4                       WildsideOutNow,JOY,RedVelvet       en      6   \n\n          location   mentions  ...  n_all_caps  n_excl_marks n_words n_urls  \\\n0              NaN        NaN  ...           0             0      22      0   \n1              NaN  larryziff  ...           0             1      24      1   \n2              NaN        NaN  ...           2             0      24      0   \n3  London, England        NaN  ...           1             1      18      0   \n4              NaN  RVsmtown0  ...          14             0      19      0   \n\n  n_mentions n_hashtags                                      text_semantic  \\\n0          0         19      sunday inspirationalquotes reproductivehealth   \n1          1         18                      luau ready marinemax sarasota   \n2          0         11  hated every minute training said quit suffer l...   \n3          0          3  mother greateful wishing happiest day hiromiyo...   \n4          1          3  joy eres la mujer talentosa que existe red vel...   \n\n   n_nouns  n_verbs  n_adjectives  \n0       19        1             2  \n1       22        0             2  \n2       19        3             1  \n3       12        2             3  \n4       12        1             3  \n\n[5 rows x 26 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>author_id</th>\n      <th>author_name</th>\n      <th>date</th>\n      <th>emails</th>\n      <th>hashtags</th>\n      <th>language</th>\n      <th>likes</th>\n      <th>location</th>\n      <th>mentions</th>\n      <th>...</th>\n      <th>n_all_caps</th>\n      <th>n_excl_marks</th>\n      <th>n_words</th>\n      <th>n_urls</th>\n      <th>n_mentions</th>\n      <th>n_hashtags</th>\n      <th>text_semantic</th>\n      <th>n_nouns</th>\n      <th>n_verbs</th>\n      <th>n_adjectives</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@Acutoronto</td>\n      <td>Acutoronto</td>\n      <td>Mar 27, 2022</td>\n      <td>0.0</td>\n      <td>motivation,acutoronto,acupuncture,tcm,fertilit...</td>\n      <td>en</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>22</td>\n      <td>0</td>\n      <td>0</td>\n      <td>19</td>\n      <td>sunday inspirationalquotes reproductivehealth</td>\n      <td>19</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>@LorraineZiff</td>\n      <td>Lorraine Ziff</td>\n      <td>Mar 26, 2022</td>\n      <td>0.0</td>\n      <td>lorraineziff,dearlorraine,smile,friendship,lua...</td>\n      <td>en</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>larryziff</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>24</td>\n      <td>1</td>\n      <td>1</td>\n      <td>18</td>\n      <td>luau ready marinemax sarasota</td>\n      <td>22</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>@ImpactWellness</td>\n      <td>Chris Caito</td>\n      <td>Mar 27, 2022</td>\n      <td>0.0</td>\n      <td>MuhammadAli,john316,isaiah4031,philippians413,...</td>\n      <td>en</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>2</td>\n      <td>0</td>\n      <td>24</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>hated every minute training said quit suffer l...</td>\n      <td>19</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>@hiromiyoshihair</td>\n      <td>Hiro Miyoshi</td>\n      <td>Mar 27, 2022</td>\n      <td>0.0</td>\n      <td>mothersday,grateful,thankful</td>\n      <td>en</td>\n      <td>0</td>\n      <td>London, England</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>mother greateful wishing happiest day hiromiyo...</td>\n      <td>12</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>@lunaxbrightwin</td>\n      <td>Liz lvs Joy</td>\n      <td>Mar 27, 2022</td>\n      <td>0.0</td>\n      <td>WildsideOutNow,JOY,RedVelvet</td>\n      <td>en</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>RVsmtown0</td>\n      <td>...</td>\n      <td>14</td>\n      <td>0</td>\n      <td>19</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>joy eres la mujer talentosa que existe red vel...</td>\n      <td>12</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 26 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweet_data.csv\")\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "n_repeats = 2\n",
    "rskf = RepeatedStratifiedKFold(n_repeats=n_repeats, n_splits=n_splits, random_state=1410)\n",
    "\n",
    "clfs = {\"MNB\" : MultinomialNB(), \"SVM\" : SVC(random_state=1234)}\n",
    "\n",
    "scores = np.zeros(shape=(3, len(clfs), n_repeats * n_splits))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metoda 1 -- TF-IDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "X = df['text_semantic'].apply(lambda txt: np.str_(txt))\n",
    "y = df['sentiment'].map({'negative' : 0, 'neutral' : 1, 'positive' : 2}).astype(int).to_numpy()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for clf_id, clf_name in enumerate(clfs):\n",
    "    for fold_id, (train_idx, test_idx) in enumerate(rskf.split(X, y)):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train = vectorizer.fit_transform(X[train_idx])\n",
    "        clf = clone(clfs[clf_name])\n",
    "        y_pred = clf.fit(X_train, y[train_idx]).predict(vectorizer.transform(X[test_idx]))\n",
    "        scores[0, clf_id, fold_id] = accuracy_score(y[test_idx], y_pred)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metoda 2 -- metadane"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "X = df[['n_words', 'n_nouns', 'n_verbs', 'n_adjectives', 'n_all_caps', 'n_excl_marks', 'n_hashtags', 'n_mentions', 'n_urls', 'emails', 'quotes', 'retweets', 'likes']].astype(int).to_numpy()\n",
    "y = df['sentiment'].map({'negative' : 0, 'neutral' : 1, 'positive' : 2}).astype(int).to_numpy()\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# MNB\n",
    "for fold_id, (train_idx, test_idx) in enumerate(rskf.split(X, y)):\n",
    "    clf = clone(clfs['MNB'])\n",
    "    y_pred = clf.fit(X[train_idx], y[train_idx]).predict(X[test_idx])\n",
    "    scores[1, 0, fold_id] = accuracy_score(y[test_idx], y_pred)\n",
    "\n",
    "# SVM\n",
    "for fold_id, (train_idx, test_idx) in enumerate(rskf.split(X, y)):\n",
    "    clf = clone(clfs['SVM'])\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X[train_idx])\n",
    "    y_pred = clf.fit(X_train, y[train_idx]).predict(scaler.transform(X[test_idx]))\n",
    "    scores[1, 1, fold_id] = accuracy_score(y[test_idx], y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metoda 3 -- mieszana"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "X = df['text'].apply(lambda txt: np.str_(txt))\n",
    "y = df['sentiment'].map({'negative' : 0, 'neutral' : 1, 'positive' : 2}).astype(int).to_numpy()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for clf_id, clf_name in enumerate(clfs):\n",
    "    for fold_id, (train_idx, test_idx) in enumerate(rskf.split(X, y)):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train = vectorizer.fit_transform(X[train_idx])\n",
    "        clf = clone(clfs[clf_name])\n",
    "        y_pred = clf.fit(X_train, y[train_idx]).predict(vectorizer.transform(X[test_idx]))\n",
    "        scores[2, clf_id, fold_id] = accuracy_score(y[test_idx], y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.675 0.671 0.678 0.673 0.673 0.702 0.641 0.666 0.663 0.682]\n",
      "  [0.666 0.648 0.669 0.667 0.638 0.685 0.651 0.667 0.646 0.643]]\n",
      "\n",
      " [[0.415 0.411 0.482 0.411 0.442 0.422 0.409 0.439 0.432 0.442]\n",
      "  [0.527 0.516 0.522 0.543 0.528 0.526 0.532 0.55  0.491 0.527]]\n",
      "\n",
      " [[0.672 0.681 0.673 0.669 0.673 0.696 0.634 0.669 0.651 0.667]\n",
      "  [0.677 0.672 0.643 0.658 0.652 0.681 0.641 0.697 0.649 0.657]]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "print(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.672 0.658]\n",
      " [0.431 0.526]\n",
      " [0.668 0.663]]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores, axis=2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analiza statystyczna"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 nan\n",
      "-----\n",
      "0 1 0.0044724039768617475\n",
      "-----\n",
      "0 2 0.008779459975274592\n",
      "-----\n",
      "0 3 0.02943243708353987\n",
      "-----\n",
      "1 0 0.0044724039768617475\n",
      "-----\n",
      "1 1 nan\n",
      "-----\n",
      "1 2 0.00675103896460789\n",
      "-----\n",
      "1 3 0.0010253512203780924\n",
      "-----\n",
      "2 0 0.008779459975274592\n",
      "-----\n",
      "2 1 0.00675103896460789\n",
      "-----\n",
      "2 2 nan\n",
      "-----\n",
      "2 3 0.015040670245087963\n",
      "-----\n",
      "3 0 0.02943243708353987\n",
      "-----\n",
      "3 1 0.0010253512203780924\n",
      "-----\n",
      "3 2 0.015040670245087963\n",
      "-----\n",
      "3 3 nan\n",
      "-----\n",
      "\n",
      "w-statistic:\n",
      "          RNN      CNN      MNB      SVM\n",
      "---  -------  -------  -------  -------\n",
      "RNN  nan       -3.762   -3.331   -2.585\n",
      "CNN    3.762  nan        3.497    4.763\n",
      "MNB    3.331   -3.497  nan        2.997\n",
      "SVM    2.585   -4.763   -2.997  nan \n",
      "\n",
      "p-value:\n",
      "          RNN      CNN      MNB      SVM\n",
      "---  -------  -------  -------  -------\n",
      "RNN  nan        0.004    0.009    0.029\n",
      "CNN    0.004  nan        0.007    0.001\n",
      "MNB    0.009    0.007  nan        0.015\n",
      "SVM    0.029    0.001    0.015  nan\n"
     ]
    }
   ],
   "source": [
    "results = np.array([[0.617, 0.652, 0.596, 0.402, 0.579, 0.626, 0.577, 0.655, 0.554, 0.670],\n",
    "                    [0.689, 0.682, 0.683, 0.671, 0.681, 0.701, 0.652, 0.680, 0.677, 0.681],\n",
    "                    [0.675, 0.671, 0.678, 0.673, 0.673, 0.702, 0.641, 0.666, 0.663, 0.682],\n",
    "                    [0.666, 0.648, 0.669, 0.667, 0.638, 0.685, 0.651, 0.667, 0.646, 0.643]])\n",
    "\n",
    "methods = [\"RNN\", \"CNN\", \"MNB\", \"SVM\"]\n",
    "\n",
    "alpha = 0.05\n",
    "w_statistic = np.zeros((len(methods), len(methods)))\n",
    "p_value = np.zeros((len(methods), len(methods)))\n",
    "\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "for i in range(len(methods)):\n",
    "    for j in range(len(methods)):\n",
    "        w_statistic[i, j], p_value[i, j] = ttest_rel(results[i], results[j])\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "headers = list(methods)\n",
    "names_column = np.expand_dims(np.array(list(methods)), axis=1)\n",
    "w_statistic_table = np.concatenate((names_column, w_statistic), axis=1)\n",
    "w_statistic_table = tabulate(w_statistic_table, headers, floatfmt=\".3f\")\n",
    "p_value_table = np.concatenate((names_column, p_value), axis=1)\n",
    "p_value_table = tabulate(p_value_table, headers, floatfmt=\".3f\")\n",
    "print(\"\\nw-statistic:\\n\", w_statistic_table, \"\\n\\np-value:\\n\", p_value_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}